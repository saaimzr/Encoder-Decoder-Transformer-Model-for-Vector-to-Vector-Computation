# Encoder-Decoder-Transformer-Model-for-Vector-to-Vector-Computation
A Transformer model built from scratch to perform basic arithmetic operations, implementing multi-head attention, feed-forward layers, and layer normalization from the Attention is All You Need paper. Trained on a toy dataset for addition and subtraction, it visualizes attention weights to show learning patterns in sequence-to-sequence tasks.
